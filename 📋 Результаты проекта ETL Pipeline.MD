# Проект ETL Pipeline с использованием Airflow, MongoDB, PostgreSQL и ClickHouse

# Итоги реализации Dockerfile для Airflow

В данном коде был разработан Dockerfile, который определяет образ для Apache Airflow, предназначенный для управления ETL процессами.

### Реализовано:

1. Основной образ:
   - Используется официальный образ Apache Airflow версии 2.6.3, который обеспечивает все необходимые компоненты для работы Airflow.

2. Установка дополнительных пакетов:
   - Образ настроен на установку необходимых пакетов, включая:
     - gcc: Компилятор для сборки программ на C.
     - python3-dev: Библиотека разработки Python, необходимая для установки некоторых Python-пакетов.
     - libpq-dev: Заголовочные файлы и библиотеки для взаимодействия с PostgreSQL.
   - Установка пакетов происходит без установки дополнительных рекомендуемых зависимостей, что позволяет уменьшить размер образа.

3. Копирование зависимостей:
   - Файл requirements.txt копируется в контейнер для установки необходимых Python-библиотек, которые могут понадобиться для работы DAG и обработки данных.

4. Установка Python-библиотек:
   - Выполняется установка зависимостей из requirements.txt с использованием pip, при этом кэширование отключено для экономии места.

5. Настройка среды Airflow:
   - Установлены переменные окружения для конфигурации Airflow:
     - Отключение загрузки примеров DAG.
     - Настройка локального исполнителя (LocalExecutor) для управления задачами.

# Итоги реализации кода Docker Compose для ETL Pipeline

В данном коде была разработана конфигурация Docker Compose, которая создает и управляет несколькими сервисами, необходимыми для реализации ETL пайплайна. 

### Реализовано:

1. MongoDB:
   - Запущен контейнер с MongoDB, настроенный с учетными данными администратора.
   - Данные хранятся в постоянном хранилище, чтобы сохранить их между перезапусками контейнера.

2. Mongo Express:
   - Установлен и запущен Mongo Express для удобного веб-интерфейса работы с MongoDB.
   - Конфигурация позволяет подключаться к MongoDB с заданными учетными данными.

3. ClickHouse:
   - Развернут контейнер ClickHouse для хранения и обработки аналитических данных.
   - Настроены учетные данные администратора и постоянное хранилище для данных и конфигураций.

4. PostgreSQL:
   - Запущен контейнер PostgreSQL с настройкой учетных данных для Airflow.
   - Включен контроль состояния контейнера с помощью healthcheck, чтобы убедиться, что база данных готова к использованию.

5. Airflow Webserver:
   - Настроен веб-сервер Airflow для управления задачами ETL.
   - Используется Dockerfile для сборки образа, а также определены переменные среды для подключения к PostgreSQL и настройки безопасности.

6. Airflow Scheduler:
   - Развернут планировщик Airflow для выполнения DAG (Directed Acyclic Graph) задач.
   - Обеспечивает выполнение задач в соответствии с установленным расписанием.

7. Grafana:
   - Установлен контейнер Grafana для визуализации данных и мониторинга показателей.
   - Настроены учетные данные администратора для доступа к интерфейсу.

8. Сеть и тома:
   - Создана общая сеть для взаимодействия всех контейнеров.
   - Определены постоянные тома для хранения данных, чтобы обеспечить их сохранность.
