# Проект ETL Pipeline с использованием Airflow, MongoDB, PostgreSQL и ClickHouse


# Итоги реализации DAG для генерации данных в MongoDB [Посмотреть код в utils.py](src/utils.py)

В данном коде был разработан Directed Acyclic Graph (DAG) для Apache Airflow, который отвечает за автоматическую генерацию тестовых данных и их сохранение в базу данных MongoDB.

### Реализовано:

1. Генерация данных:
   - Определены две функции для создания случайных данных:
     - generate_user_session(): Создает данные о сессиях пользователей с уникальными идентификаторами, временными метками и информацией о действиях.
     - generate_support_ticket(): Создает данные о тикетах поддержки с уникальными идентификаторами, статусами и типами проблем.

2. Подключение к MongoDB:
   - Используется клиент MongoDB (mongo_client) для записи сгенерированных данных в коллекции базы данных raw_data.

3. Настройка DAG:
   - Создан DAG с именем generate_mongo_data, который запускается каждые 10 минут.
   - Включает задачу generate_mongo_data, вызывающую функцию generate_data(), генерирующую и сохраняющую данные в MongoDB.

4. Обработка данных:
   - Генерируется 1000 строк данных как для пользовательских сессий, так и для тикетов поддержки, которые затем вставляются в соответствующие коллекции.


# Итоги реализации подключения к базам данных

В данном коде реализовано подключение к трем различным базам данных, которые используются в рамках ETL процессов для Apache Airflow.

### Реализовано:

1. Импорт необходимых библиотек:
   - Импортированы библиотеки для работы с датами, MongoDB, PostgreSQL и ClickHouse:
     - datetime: Для работы с временными метками.
     - pymongo: Для взаимодействия с MongoDB.
     - psycopg2: Для работы с PostgreSQL.
     - clickhouse_driver: Для подключения к ClickHouse.

2. Настройка параметров по умолчанию:
   - Определены параметры по умолчанию для DAG, включая владельца и дату начала.

3. Подключение к MongoDB:
   - Установлено соединение с локальным экземпляром MongoDB, используя учетные данные администратора.

4. Подключение к PostgreSQL:
   - Установлено соединение с базой данных PostgreSQL с использованием учетных данных пользователя airflow.

5. Подключение к ClickHouse:
   - Создан клиент для подключения к ClickHouse с использованием учетных данных администратора.
# Итоги реализации Dockerfile для Airflow

В данном коде был разработан Dockerfile, который определяет образ для Apache Airflow, предназначенный для управления ETL процессами.

### Реализовано:

1. Основной образ:
   - Используется официальный образ Apache Airflow версии 2.6.3, который обеспечивает все необходимые компоненты для работы Airflow.

2. Установка дополнительных пакетов:
   - Образ настроен на установку необходимых пакетов, включая:
     - gcc: Компилятор для сборки программ на C.
     - python3-dev: Библиотека разработки Python, необходимая для установки некоторых Python-пакетов.
     - libpq-dev: Заголовочные файлы и библиотеки для взаимодействия с PostgreSQL.
   - Установка пакетов происходит без установки дополнительных рекомендуемых зависимостей, что позволяет уменьшить размер образа.

3. Копирование зависимостей:
   - Файл requirements.txt копируется в контейнер для установки необходимых Python-библиотек, которые могут понадобиться для работы DAG и обработки данных.

4. Установка Python-библиотек:
   - Выполняется установка зависимостей из requirements.txt с использованием pip, при этом кэширование отключено для экономии места.

5. Настройка среды Airflow:
   - Установлены переменные окружения для конфигурации Airflow:
     - Отключение загрузки примеров DAG.
     - Настройка локального исполнителя (LocalExecutor) для управления задачами.


# Итоги реализации кода Docker Compose для ETL Pipeline

В данном коде была разработана конфигурация Docker Compose, которая создает и управляет несколькими сервисами, необходимыми для реализации ETL пайплайна. 

### Реализовано:

1. MongoDB:
   - Запущен контейнер с MongoDB, настроенный с учетными данными администратора.
   - Данные хранятся в постоянном хранилище, чтобы сохранить их между перезапусками контейнера.

2. Mongo Express:
   - Установлен и запущен Mongo Express для удобного веб-интерфейса работы с MongoDB.
   - Конфигурация позволяет подключаться к MongoDB с заданными учетными данными.

3. ClickHouse:
   - Развернут контейнер ClickHouse для хранения и обработки аналитических данных.
   - Настроены учетные данные администратора и постоянное хранилище для данных и конфигураций.

4. PostgreSQL:
   - Запущен контейнер PostgreSQL с настройкой учетных данных для Airflow.
   - Включен контроль состояния контейнера с помощью healthcheck, чтобы убедиться, что база данных готова к использованию.

5. Airflow Webserver:
   - Настроен веб-сервер Airflow для управления задачами ETL.
   - Используется Dockerfile для сборки образа, а также определены переменные среды для подключения к PostgreSQL и настройки безопасности.

6. Airflow Scheduler:
   - Развернут планировщик Airflow для выполнения DAG (Directed Acyclic Graph) задач.
   - Обеспечивает выполнение задач в соответствии с установленным расписанием.

7. Grafana:
   - Установлен контейнер Grafana для визуализации данных и мониторинга показателей.
   - Настроены учетные данные администратора для доступа к интерфейсу.

8. Сеть и тома:
   - Создана общая сеть для взаимодействия всех контейнеров.
   - Определены постоянные тома для хранения данных, чтобы обеспечить их сохранность.
