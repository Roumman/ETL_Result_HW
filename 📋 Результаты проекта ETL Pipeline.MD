# Проект ETL Pipeline с использованием Airflow, MongoDB, PostgreSQL и ClickHouse


# Итоги реализации DAG для генерации данных в MongoDB 
[Посмотреть код в MongoDB Data Generation DAG.py](scr/MongoDB%20Data%20Generation%20DAG.py)

В данном коде был разработан Directed Acyclic Graph (DAG) для Apache Airflow, который отвечает за автоматическую генерацию тестовых данных и их сохранение в базу данных MongoDB.

### Реализовано:

1. Генерация данных:
   - Определены две функции для создания случайных данных:
     - generate_user_session(): Создает данные о сессиях пользователей с уникальными идентификаторами, временными метками и информацией о действиях.
     - generate_support_ticket(): Создает данные о тикетах поддержки с уникальными идентификаторами, статусами и типами проблем.

2. Подключение к MongoDB:
   - Используется клиент MongoDB (mongo_client) для записи сгенерированных данных в коллекции базы данных raw_data.

3. Настройка DAG:
   - Создан DAG с именем generate_mongo_data, который запускается каждые 10 минут.
   - Включает задачу generate_mongo_data, вызывающую функцию generate_data(), генерирующую и сохраняющую данные в MongoDB.

4. Обработка данных:
   - Генерируется 1000 строк данных как для пользовательских сессий, так и для тикетов поддержки, которые затем вставляются в соответствующие коллекции.


# Итоги реализации подключения к базам данных
[Посмотреть код в Database Connection Setup for ETL Pipeline.py](https://github.com/вашеимя/ETL_Result_HW/blob/main/scr/Database%20Connection%20Setup%20for%20ETL%20Pipeline.py)

В данном коде реализовано подключение к трем различным базам данных, которые используются в рамках ETL процессов для Apache Airflow.

### Реализовано:

1. Импорт необходимых библиотек:
   - Импортированы библиотеки для работы с датами, MongoDB, PostgreSQL и ClickHouse:
     - datetime: Для работы с временными метками.
     - pymongo: Для взаимодействия с MongoDB.
     - psycopg2: Для работы с PostgreSQL.
     - clickhouse_driver: Для подключения к ClickHouse.

2. Настройка параметров по умолчанию:
   - Определены параметры по умолчанию для DAG, включая владельца и дату начала.

3. Подключение к MongoDB:
   - Установлено соединение с локальным экземпляром MongoDB, используя учетные данные администратора.

4. Подключение к PostgreSQL:
   - Установлено соединение с базой данных PostgreSQL с использованием учетных данных пользователя airflow.

5. Подключение к ClickHouse:
   - Создан клиент для подключения к ClickHouse с использованием учетных данных администратора.


# Итоги реализации Dockerfile для Airflow
[Посмотреть код в Apache Airflow Dockerfile for ETL Processes.py](scr/Apache%20Airflow%20Dockerfile%20for%20ETL%20Processes.py)

В данном коде был разработан Dockerfile, который определяет образ для Apache Airflow, предназначенный для управления ETL процессами.

### Реализовано:

1. Основной образ:
   - Используется официальный образ Apache Airflow версии 2.6.3, который обеспечивает все необходимые компоненты для работы Airflow.

2. Установка дополнительных пакетов:
   - Образ настроен на установку необходимых пакетов, включая:
     - gcc: Компилятор для сборки программ на C.
     - python3-dev: Библиотека разработки Python, необходимая для установки некоторых Python-пакетов.
     - libpq-dev: Заголовочные файлы и библиотеки для взаимодействия с PostgreSQL.
   - Установка пакетов происходит без установки дополнительных рекомендуемых зависимостей, что позволяет уменьшить размер образа.

3. Копирование зависимостей:
   - Файл requirements.txt копируется в контейнер для установки необходимых Python-библиотек, которые могут понадобиться для работы DAG и обработки данных.

4. Установка Python-библиотек:
   - Выполняется установка зависимостей из requirements.txt с использованием pip, при этом кэширование отключено для экономии места.

5. Настройка среды Airflow:
   - Установлены переменные окружения для конфигурации Airflow:
     - Отключение загрузки примеров DAG.
     - Настройка локального исполнителя (LocalExecutor) для управления задачами.


# Итоги реализации кода Docker Compose для ETL Pipeline
[Посмотреть код в ETL Pipeline Docker Compose Configuration.py](scr/ETL%20Pipeline%20Docker%20Compose%20Configuration.py)

В данном коде была разработана конфигурация Docker Compose, которая создает и управляет несколькими сервисами, необходимыми для реализации ETL пайплайна. 

### Реализовано:

1. MongoDB:
   - Запущен контейнер с MongoDB, настроенный с учетными данными администратора.
   - Данные хранятся в постоянном хранилище, чтобы сохранить их между перезапусками контейнера.

2. Mongo Express:
   - Установлен и запущен Mongo Express для удобного веб-интерфейса работы с MongoDB.
   - Конфигурация позволяет подключаться к MongoDB с заданными учетными данными.

3. ClickHouse:
   - Развернут контейнер ClickHouse для хранения и обработки аналитических данных.
   - Настроены учетные данные администратора и постоянное хранилище для данных и конфигураций.

4. PostgreSQL:
   - Запущен контейнер PostgreSQL с настройкой учетных данных для Airflow.
   - Включен контроль состояния контейнера с помощью healthcheck, чтобы убедиться, что база данных готова к использованию.

5. Airflow Webserver:
   - Настроен веб-сервер Airflow для управления задачами ETL.
   - Используется Dockerfile для сборки образа, а также определены переменные среды для подключения к PostgreSQL и настройки безопасности.

6. Airflow Scheduler:
   - Развернут планировщик Airflow для выполнения DAG (Directed Acyclic Graph) задач.
   - Обеспечивает выполнение задач в соответствии с установленным расписанием.

7. Grafana:
   - Установлен контейнер Grafana для визуализации данных и мониторинга показателей.
   - Настроены учетные данные администратора для доступа к интерфейсу.

8. Сеть и тома:
   - Создана общая сеть для взаимодействия всех контейнеров.
   - Определены постоянные тома для хранения данных, чтобы обеспечить их сохранность.


## Итоги реализации DAG для создания таблиц и переноса данных
[Посмотреть код в ETL Data Transfer to ClickHouse](scr/ETL%20Data%20Transfer%20to%20ClickHouse.py)

В данном коде реализован Directed Acyclic Graph (DAG) для Apache Airflow, который отвечает за создание таблиц в ClickHouse и перенос данных из PostgreSQL в ClickHouse.

##№ Реализовано:

1. Импорт необходимых библиотек:
   - Импортированы библиотеки для работы с Airflow, PostgreSQL и ClickHouse, а также для обработки дат и логирования.

2. Создание таблиц:
   - Определена функция create_tables(), которая содержит SQL-запросы для создания трех таблиц в ClickHouse:
     - user_activity_daily: Таблица для хранения ежедневной активности пользователей.
     - support_performance: Таблица для хранения производительности службы поддержки.
     - activity_tickets: Таблица для учета активности пользователей и тикетов.

3. Перенос данных:
   - Функция transfer_data() выполняет следующие операции:
     - Извлечение данных из таблицы user_sessions в PostgreSQL с агрегацией по дате и пользователю, затем вставка результатов в таблицу user_activity_daily в ClickHouse.
     - Извлечение данных из таблицы support_tickets для анализа производительности и вставка их в таблицу support_performance.
     - Извлечение данных о пользователях и тикетах, агрегация этих данных и вставка результатов в таблицу activity_tickets.

4. Настройка DAG:
   - Создан DAG с именем clickhouse_dwh, который запускается ежедневно.
   - Определена задача transfer_data, которая вызывает функцию transfer_data() для выполнения переноса данных.



## Итоги реализации DAG для миграции данных из MongoDB в PostgreSQL
[Посмотреть код в MongoDB to PostgreSQL Data Migration DAG.py](scr/Mongo%20DB%20to%20PostgreSQL%20Data%20Migration%20DAG.py)

В данном коде реализован Directed Acyclic Graph (DAG) для Apache Airflow, который отвечает за миграцию данных из MongoDB в PostgreSQL.

### Реализовано:

1. Импорт необходимых библиотек:
   - Импортированы библиотеки для работы с Airflow, MongoDB и PostgreSQL, а также для работы с датами и JSON.

2. Определение функции миграции:
   - Функция migrate_user_sessions() отвечает за миграцию данных:
     - Извлечение данных о пользовательских сессиях и тикетах поддержки из коллекций MongoDB user_sessions и support_tickets, соответственно.
     - Фильтрация данных за последний час с использованием временных меток.

3. Создание таблиц (в комментариях):
   - В коде содержатся SQL-запросы для создания таблиц в PostgreSQL, если они не существуют. Это включает таблицы для пользовательских сессий и тикетов поддержки.

4. Перенос данных в PostgreSQL:
   - Используя курсор для выполнения запросов, данные из MongoDB записываются в таблицы PostgreSQL:
     - Данные о сессиях пользователей вставляются в таблицу replicated_data.user_sessions.
     - Данные о тикетах поддержки вставляются в таблицу replicated_data.support_tickets.
   - Используется конструкция ON CONFLICT DO NOTHING, чтобы избежать дублирования записей.

5. Настройка DAG:
   - Создан DAG с именем mongo_to_postgres, который запускается каждый час.
   - Определена задача migrate_user_sessions, которая вызывает функцию migrate_user_sessions() для выполнения миграции данных.
